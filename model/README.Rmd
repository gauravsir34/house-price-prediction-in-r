---
output: github_document
---

```{r setup, include = FALSE}
# The README markdown generate by this rmd file, please edit the content within this R markdown.

knitr::opts_chunk$set(
  collapse = TRUE,
  message = FALSE,
  error = FALSE,
  warning = FALSE,
  fig.path = "../plot/",
  fig.align = "center",
  echo = FALSE
)
```

```{r data}
# load library
library(tidyverse)
library(patchwork)

# MAE
MAE <- function(obs, pred) {
  e <- obs - pred
  mae <- mean(abs(e))
  return(mae)
}

# Import the spilt training data set
train_85 <- read_csv("../data/train_85.csv")
train_15 <- read_csv("../data/train_15.csv")
```

# Melbourne House Price Prediction - Data Modeling

### Data Files

* [`model_decision_tree.R`](https://github.com/Jiaying-Wu/Melbourne-House-Price-Prediction/blob/master/model/model_decision_tree.R): R Script of Decision Tree Model.

* [`model_random_forest.R`](https://github.com/Jiaying-Wu/Melbourne-House-Price-Prediction/blob/master/model/model_random_forest.R): R Script of Random Forest Model.

* [`model_gbm.R`](https://github.com/Jiaying-Wu/Melbourne-House-Price-Prediction/blob/master/model/model_gbm.R): R Script of Gradient Boosting Model.

## Data Modeling

In this competition, we focus on these 3 tree-base models:

* Decision Tree

* Random Forest

* Gradient Boosting

#### Decision Tree

The first type of model we build is Decision Tree, using the package `rpart`. 

Source code `model_decision_tree.R` under `model` folder.

This model has a small cp allowed to grown a deep tree, then setting minspilt as 70. The node at least to have 70 samples to continue splitting, in order to reduce overfitting.  

```{r dt_model_visul}
dt <- readRDS("saved_model/dt_base.RDS")
plot(dt)
```

That is 751 end leaves, that is 751 prediction value for 32202 training samples.

```{r dt_size}
NROW(dt$y)
dt$frame %>%
  filter(var == "<leaf>") %>%
  nrow()
```

The size of samples in the end leaves between 23 to 69.

```{r size_endleaf_dt}
dt$frame %>%
  filter(var == "<leaf>") %>%
  ggplot(aes(x = n)) +
  geom_bar() +
  xlab("sample size") +
  ggtitle("Count of sample size in the end leaf in Decision Tree model")
```

Variable importance:

```{r dt_imp}
dt$variable.importance
```

The result of this decision tree model suggested variables `house_size`, `id` and `suburb` are the top 3 affecting factors of the Melbourne house price.

```{r dt_mae}
MAE(predict(dt, train_15), train_15$price)
```

The Mean Absolute Error (MAE) of the local test set is `1.216002`, it estimated the average error in out of sample prediction for Melbourne house price is `AUD121,600.2`

#### Random Forest

The second type of model we build is Random Forest, using the package `randomForest` and parameters tunning using package `caret`.

Source code `model_random_forest.R` under `model` folder.

##### Parameters tunning

First we tuned the `mtry` between 1 to 13 base on 500 trees, with 5-fold cross validation and repeat 2 times. `mtry` is the number of variables to consider in each split.

The MAE of each parameter is:

![](../plot/plot_rf_grid_mtry.png)

It suggested the optimize `mtry` base on 500 tree is 13

Then we tuned the parameter `ntree` base on `mtry = 13` on this sequence of tree: `500, 1000, 1500, 2000, 3000`, with 5-fold cross validation and repeat 2 times.. `ntree` is the number of trees to grow.

The plot of tunning result is:

![](../plot/rf_grid_ntree.png)

The summary of the tunning result is:

```{r rf_message}
readRDS("saved_message/rf_grid_ntree_error_RDS")
```

Base on the median MAE and mean MAE, it suggested `ntree = 2000`

Base on grid search, the optimized parameters for fitting random forest model in this dataset is `mtry = 13, ntree = 2000`.

The variable importance plot base on the optimize Random Forest model is:

![](../plot/optimise_rf_var_imp.png)

It suggested the top affecting 3 factors for Melbourne house price in this model is `house_size`, `id`, `ncars`.

The Mean Absolute Error (MAE) of the local test set is `1.334874`, it estimated the average error in out of sample prediction for Melbourne house price is `AUD133,487.4`.

#### Gradient Boosting

The third type of model we build is Gradient Boosting, using the package `gbm`. We build two model with two different distribution `gaussian` and `laplace`.

Source code `model_gbm.R` under `model` folder.

The reason we choose this two distribution was the distribution of Melbourne house price has a first increasing exponential then decreasing exponential, share the same property of laplace distribution. The house price after taking the log follow the gaussian distribution.

```{r, price_distribution,fig.height=4,fig.width=9}
price_his <- train_85 %>%
  ggplot(aes(x = price)) +
  geom_histogram()

log_price_his <- train_85 %>%
  ggplot(aes(x = log(price))) +
  geom_histogram()

price_his + log_price_his
```

##### Gradient Boosting with gaussian distribution

We train this Gradient Boosting model with the following parameters:

```
gbm_gaussian <- gbm(log_price~.-price, 
                 data = train_85,
                 distribution = "gaussian",
                 n.trees = 1000,
                 shrinkage=0.01,
                 interaction.depth = 20,
                 n.minobsinnode = 30,
                 bag.fraction = 0.8,
                 train.fraction = 0.8,
                 cv.folds = 4,
                 n.cores = 4)
```

Choosing the number of trees base on cross validation, it was 752.

![](../plot/gbm_gaussian_cv_tree.png)

The feature important plot suggested `id` is the most important variable.

![](../plot/gbm_gaussian_cv_important.png)

The Mean Absolute Error (MAE) of the local test set is `1.413301`, it estimated the average error in out of sample prediction for Melbourne house price is `AUD141,330.1`.

##### Gradient Boosting with laplace distribution

We train this Gradient Boosting model with the following parameters:

```
gbm_laplace <- gbm(price~.-log_price, 
                    data = train_85,
                    distribution = "laplace",
                    n.trees = 1000,
                    shrinkage = 0.028,
                    interaction.depth = 21,
                    n.minobsinnode = 30,
                    bag.fraction = 0.8,
                    train.fraction = 0.8,
                    cv.folds = 4,
                    n.cores = 4)
```

Choosing the number of trees base on cross validation, it was 990.

![](../plot/gbm_laplace_cv_tree.png)

The feature important plot suggested `suburb` is the most important variable.

![](../plot/gbm_laplace_cv_important.png)

The Mean Absolute Error (MAE) of the local test set is `1.22285`, it estimated the average error in out of sample prediction for Melbourne house price is `AUD122,285`.

